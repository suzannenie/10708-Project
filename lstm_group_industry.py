# -*- coding: utf-8 -*-
"""lstm_group_industry.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mbsm76GqDGA3XonjtIKV8KxcEBw4u7P-
"""

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/10708/

!pip install kaggle

# Commented out IPython magic to ensure Python compatibility.
# %cp .kaggle/kaggle.json /root/.kaggle/kaggle.json

!kaggle datasets download -d paultimothymooney/stock-market-data

!unzip *.zip

"""# Group By Sector"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

df = pd.read_csv("sectors/all_industries.csv")

len(df)

sectors = []
for group in df.groupby(['Sector']):
  sectors.append(group[0])
tickers = {}
for sector in sectors:
  tickers[sector] = set()

tickers

for row in df.iterrows():
  try:
    tickers[row[1]['Sector']].add(row[1]['Symbol'])
  except:
    print(row)

total = 0
for sector in tickers:
  print(sector, len(tickers[sector]))
  total += len(tickers[sector])
print(total)

with open('sector_ticker_sets.pkl', 'wb') as f:
  pickle.dump(tickers, f)

import glob

files = glob.glob('stock_market_data/nasdaq/csv/*.csv')
len(files), files[0].split('/')[-1][:-4]
not_found = 0
tech = []

provided = {}
for filename in files:
  name = filename.split('/')[-1][:-4]
  found = False
  for sector in tickers:
    if name in tickers[sector]:
      found = True
      provided[sector] = provided.get(sector, 0) + 1
    if name in tickers["Technology"]:
      tech.append(name)
  if not found:
    not_found += 1
    # print(name)
print(not_found, len(files))

import matplotlib.pyplot as plt

D = provided

plt.bar(range(len(D)), list(D.values()), align='center')
plt.xticks(range(len(D)), list(D.keys()), rotation=90)
plt.title("Distribution by Sector NASDAQ")

plt.show()

import glob

files = glob.glob('stock_market_data/nyse/csv/*.csv')
len(files), files[0].split('/')[-1][:-4]
not_found = 0
tech = []

provided = {}
for filename in files:
  name = filename.split('/')[-1][:-4]
  found = False
  for sector in tickers:
    if name in tickers[sector]:
      found = True
      provided[sector] = provided.get(sector, 0) + 1
      break
    if name in tickers["Technology"]:
      tech.append(name)
  if not found:
    not_found += 1
    # print(name)
print(not_found, len(files))

import matplotlib.pyplot as plt

D = provided

plt.bar(range(len(D)), list(D.values()), align='center')
plt.xticks(range(len(D)), list(D.keys()), rotation=90)
plt.title("Distribution by Sector NYSE")

plt.show()

provided

tech = set(tech)
len(tech)

import pickle

# with open('tech_set_nyse.pkl', 'wb') as f:
#   pickle.dump(tech, f)
with open('tech_set_nyse.pkl', 'rb') as f:
  tech_nyse = pickle.load(f)
with open('tech_set.pkl', 'rb') as f:
  tech_nasdaq = pickle.load(f)

import pandas as pd
import os
import torch
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
class CSVData(Dataset):
    def __init__(self, folder_path, tech, training_length, predict_length):
        self.folder_path = folder_path
        self.training_length = training_length
        self.predict_length = predict_length
        # self.files = [f for f in os.listdir(folder_path) if f.endswith('.csv')][:200]
        self.files = []
        for ticker in tech:
          self.files.append(ticker + '.csv')
        self.data = []
        self.tickers = []
        self.ticker_list = []
        print(len(self.files))

        for file in self.files:
          try:
            csv_file = pd.read_csv(os.path.join(folder_path, file))
            open_col = csv_file['Open'].values
            self.tickers.append(file.split('.')[0])
            ticker = file.split('.')[0]
            for i in range(len(open_col) - training_length - predict_length):
                self.data.append((open_col[i:i+training_length], open_col[i+training_length:i+training_length+predict_length], ticker))
          except Exception as e:
            print(e, print(file.split('.')[0]))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        training_data, label_data, ticker = self.data[index]
        training_data = torch.Tensor(training_data)
        label_data = torch.Tensor(label_data)
        # ticker = torch.Tensor(ticker)
        return training_data, label_data, ticker

    def plot_trend(self, batch_data):
        plt.figure(figsize=(10, 5))
        for i in range(batch_data.size(0)):
            x = torch.arange(batch_data.shape[1])
            y = batch_data[i].numpy()  * 100
            plt.plot(x, y, label=f'{self.tickers[i]}')
        plt.xlabel('Days')
        plt.ylabel('Percentage of Original Value')
        plt.title('Stock Trend')
        plt.legend()
        plt.show()

# Example usage
folder_path = "stock_market_data/nasdaq/csv/"
training_length = 60
predict_length = 30

dataset = CSVData(folder_path, tech_nasdaq, training_length, predict_length)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# Iterate through batches of data
i = 0
for batch in dataloader:
    training_data, label_data, ticker = batch
    if i == 3:
      print(training_data.shape, label_data.shape)
      dataset.plot_trend(training_data)
      break
    i += 1

max_seq_len = 100
batch_size = 16

# Example usage
folder_path = "stock_market_data/nasdaq/csv/"
training_length = 60
predict_length = 30

dataset = CSVData(folder_path, tech_nasdaq, training_length, predict_length)
train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

import numpy as np

for data in train_loader:
    
    column_data, labels, tickers = data
    print(column_data.shape, labels.shape)
    # train_loader.plot_trend(column_data, column_len)
    corr_matrix = torch.corrcoef(column_data.squeeze(-1))

    # display correlation matrix
    # plt.matshow(corr_matrix, 0)
    print(dataset.tickers)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(corr_matrix)
    fig.colorbar(cax)
    
    ax.set_xticklabels(['']+list(tickers))
    ax.set_yticklabels(['']+list(tickers))

    plt.show()
    break

# Define hyperparameters
batch_size = 8
learning_rate = 1e-20
hidden_size = 50
training_length = 60
predict_length = 30

train_folder_path = 'stock_market_data/nasdaq/csv/'
test_folder_path = 'stock_market_data/nyse/csv/'
train_dataset = CSVData(train_folder_path, tech_nasdaq, training_length, predict_length)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

test_dataset = CSVData(test_folder_path, tech_nyse, training_length, predict_length)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

len(train_loader), len(test_loader)

import pickle

with open('tech_loader.pkl', 'wb') as f:
  pickle.dump(test_loader, f)

with open('tech_loader.pkl', 'rb') as f:
  p = pickle.load( f)
len(p)

import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

# Define the sequence length for input data to the LSTM model
sequence_length = 60
prediction_length = 30

# Define the LSTM model
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, seq_len, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, input_size, num_layers, batch_first=True)
        self.linear1 = nn.Linear(seq_len, hidden_size)
        self.fc = nn.Linear(seq_len, prediction_length)
        self.fc1 = nn.Linear(seq_len, seq_len)
        self.fc2 = nn.Linear(seq_len, seq_len)

        self.fc_only_lstm = nn.Linear(hidden_size, prediction_length)
        self.gelu = nn.GELU()
        self.seq_len = seq_len

    def forward(self, x, adj_matrix):
        out, _ = self.lstm(x)
        # out2, _ = self.lstm2(out)
        # # out2 = out2.reshape(self.seq_len, -1)
        # tmp = torch.matmul(adj_matrix, out2)
        # # tmp = tmp.reshape(-1, self.seq_len)
        # tmp_out = self.fc1(tmp)
        # gelu1_out = self.gelu(tmp_out)

        # tmp = torch.matmul(adj_matrix, gelu1_out)
        # tmp_out = self.fc2(tmp)
        # gelu1_out = self.gelu(tmp_out)

        out = self.fc_only_lstm(out.reshape(x.shape[0], -1))
        return out

learning_rate = 1e-6
device = 'cuda'

model = StockLSTM(input_size=60, hidden_size=hidden_size, seq_len=training_length).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = nn.MSELoss().to(device)
overall_avg_loss = []
overall_test_loss = []

import numpy as np
import pandas as pd

def create_adj_matrix(data, threshold = 0.8):
    """
    Creates an adjacency matrix for a correlation graph given a pandas DataFrame of stock prices and a threshold value.

    :param data: pandas DataFrame of stock prices with each column representing a stock and each row representing a day
    :param threshold: minimum correlation coefficient value for an edge to be included in the graph
    :return: a numpy ndarray representing the adjacency matrix
    """
    # Calculate the correlation matrix
    # print(data.squeeze(-1))
    corr_matrix = torch.corrcoef(data.squeeze(-1))

    nan_indices = torch.isnan(corr_matrix)
    corr_matrix[nan_indices] = 0.0

    # adj_matrix = np.where(np.abs(corr_matrix) >= threshold, 1, 0)
    # return torch.from_numpy(adj_matrix).float()

    return corr_matrix

  

from tqdm import tqdm
num_epochs = 8

from sklearn.preprocessing import normalize
import numpy as np


def scale(X):
  '''
  Scales each row so that the largest value is 1.0
  '''
  eps = 1e-12
  scaled_X = (X - 0) / (torch.max(X, axis=1).values - 0 + eps).unsqueeze(1)
  return scaled_X, torch.max(X, axis=1).values.unsqueeze(1)

for epoch in range(num_epochs):
    print('epoch', epoch)
    running_loss = 0.0
    path = f'{epoch}_random_epoch_new.ckpt'
    torch.save(model.state_dict(), path)
    # Wrap the train_loader with tqdm for progress bar
    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):
        
        column_data, labels, tickers = data
        optimizer.zero_grad()
        if torch.sum(torch.isnan(column_data)) + torch.sum(torch.isnan(labels))> 0:
          continue

        column_data, scaling_factor = scale(column_data)
        
        adj_matrix = create_adj_matrix(column_data)
        column_data = column_data.to(device)
        outputs = model(column_data, adj_matrix)

        labels = labels.to(device)
        outputs= outputs.to(device)

        labels_scaled = torch.clamp(labels / (scaling_factor.to(device) + 1e-12), 0.0, 1.0)
        loss = loss_fn(outputs, labels_scaled)
        # print(loss.item())

        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        if i % 10000 == 99 :
          print(running_loss / (i + 1))
        # Print the average loss every 100 batches
        # if i % 100 == 99:
        #     model.eval()
        #     test_loss = 0.0
        #     for test_data in test_loader:
        #         column_data_test, y_test = test_data
        #         optimizer.zero_grad()
        #         adj_matrix_test = create_adj_matrix(column_data_test)

        #         y_pred = model(column_data, adj_matrix_test)
        #         test_loss += loss_fn(y_pred, y_test)

        #     print(test_loss.item() / len(test_loader))
        #     overall_test_loss.append(test_loss.item() / len(test_loader))
        #     print(f"Epoch {epoch + 1}, Batch {i + 1}: Average Loss = {running_loss / 100}")
        #     overall_avg_loss.append(running_loss / 100)
        #     running_loss = 0.0
        #     model.train()
            
    # Print the average loss for the epoch
    avg_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch + 1}: Average Loss = {avg_loss}")
    overall_avg_loss.append(avg_loss)

# path = '3_epochs.ckpt'
# torch.save(model.state_dict(), path)
# model.load_state_dict(torch.load(path))

tech_model = StockLSTM(input_size=60, hidden_size=hidden_size, seq_len=training_length).to(device)
tech_model.load_state_dict(torch.load('7_epoch_new.ckpt'))
random_model = StockLSTM(input_size=60, hidden_size=hidden_size, seq_len=training_length).to(device)
random_model.load_state_dict(torch.load('7_random_epoch_new.ckpt'))

with open('test_loader.pkl', 'rb') as f:
  random_test = pickle.load(f)
len(random_test)

model = random_model
test_loss = 0.0
model.eval()
test_loader = random_test

test_loss = 0.0
with torch.no_grad():
  for i, test_data in tqdm(enumerate(test_loader), total=len(test_loader)):
      column_data_test, y_test, tickers = test_data
      column_data_test = column_data_test.to(device)
      y_test = y_test.to(device)
      column_data_test, scaling_factor = scale(column_data_test)
      if torch.sum(torch.isnan(column_data_test)) + torch.sum(torch.isnan(y_test))> 0:
          continue
      # optimizer.zero_grad()
      adj_matrix_test = create_adj_matrix(column_data_test)
      y_pred = model(column_data_test, adj_matrix_test) 
      y_test_clamp = torch.clamp(y_test / (scaling_factor.to(device) + 1e-12), 0.0, 1.0)

      loss = loss_fn(y_pred, y_test_clamp)
      test_loss += loss.item()
      if np.isnan(test_loss):
        print(column_data_test)
        print(y_pred, y_test_clamp)
        break
      if i % 10000 == 999 :
          print(test_loss / (i + 1))
          y_pred *= scaling_factor.to(device)
          # break
      # print(test_loss)
      # break
print(test_loss / len(test_loader))

print(test_loss / len(test_loader))

model = random_model
test_loss = 0.0
model.eval()
test_loader = random_test

test_loss = 0.0
with torch.no_grad():
  i = 0
  prev_y_test = []
  for i, test_data in tqdm(enumerate(test_loader), total=len(test_loader)):
      column_data_test, y_test, tickers = test_data
      column_data_test = column_data_test.to(device)
      y_test = y_test.to(device)
      prev_y_test.append(y_test)
      if len(prev_y_test) > 3:
        prev_y_test = prev_y_test[1:]
        
      column_data_test, scaling_factor = scale(column_data_test)
      if torch.sum(torch.isnan(column_data_test)) + torch.sum(torch.isnan(y_test))> 0:
          continue
      # optimizer.zero_grad()
      adj_matrix_test = create_adj_matrix(column_data_test)
      y_pred = model(column_data_test, adj_matrix_test) 
      y_test_clamp = torch.clamp(y_test / (scaling_factor.to(device) + 1e-12), 0.0, 1.0)

      loss = loss_fn(y_pred, y_test_clamp)
      test_loss += loss.item()
      if np.isnan(test_loss):
        print(column_data_test)
        print(y_pred, y_test_clamp)
        break
      if i % 10000 == 999 :
          print(test_loss / (i + 1))
          y_pred *= scaling_factor.to(device)
          # break
      # print(test_loss)
      # break
print(test_loss / len(test_loader))

test_loss/len(test_loader)

for x in prev_y_test:
  print(x.shape)

all_y_test = np.concatenate(prev_y_test, axis=1)

import matplotlib.pyplot as plt

print(ticker)
# Plot the true values in blue
plt.plot([i for i in range(90)], all_y_test[0], color='blue', label='True Values')

# Plot the predicted values in red
plt.plot([i for i in range(60, 90)], y_pred.cpu().numpy()[0], color='red', label='Predicted Values')

# Add axis labels and a legend
plt.title(f'{ticker[0]} tech stock prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()

# Show the plot
plt.show()

